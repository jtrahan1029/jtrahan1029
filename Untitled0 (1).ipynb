{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Disaster Detection Tweets\n",
        "\n",
        "### For this assignment, I will be predicting if a tweet relates to some natural or personal disaster or not. Many tweets may have words that a machine may associate with a disaster, such as wreck, but are actually exclamations. \n",
        "\n",
        "### I will be converting tweets into a sequence of encoded words and using those endoded sequense to determine if a tweet is about an actual disaster.\n"
      ],
      "metadata": {
        "id": "IX9rdIPpcekC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the packages that will be used"
      ],
      "metadata": {
        "id": "1b8r3nx2daYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n"
      ],
      "metadata": {
        "id": "5-zSxiblWSmS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "from time import time\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from keras.utils.data_utils import pad_sequences\n",
        "from keras.layers import preprocessing"
      ],
      "metadata": {
        "id": "8QjxNS4LZiPz"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "nOLS-Q3PetUd"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q kaggle"
      ],
      "metadata": {
        "id": "ajytkcH6Wcrx"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q opendatasets"
      ],
      "metadata": {
        "id": "DymeivPVlEiX"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bJkjxjpXe-P",
        "outputId": "6c42aeeb-1245-4596-c3c8-0b67804899eb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.8/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.26.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.25.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "H7sppS1cfF66"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UX3q4ijV8dt",
        "outputId": "8ac34024-68a9-46ab-d808-6b38488e9c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: joshuatrahan\n",
            "Your Kaggle Key: ··········\n",
            "Downloading nlp-getting-started.zip to ./nlp-getting-started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 593k/593k [00:00<00:00, 77.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting archive ./nlp-getting-started/nlp-getting-started.zip to ./nlp-getting-started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "\n",
        "od.download(\"https://www.kaggle.com/competitions/nlp-getting-started/data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update the cwd and create dataframes for the train and test data. Get value counts for outputs."
      ],
      "metadata": {
        "id": "d0kfGGApdiPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curr_dir = os.chdir('/content/nlp-getting-started')\n",
        "\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./test.csv')\n",
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHyRSqIQXxoY",
        "outputId": "0bc7bacc-c0f5-4f41-a8c2-0212576a134c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pXXJOPDA7Ku9",
        "outputId": "60f64394-8291-4385-d965-1031761c4fdc"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n",
              "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
              "2   5     NaN      NaN  all residents asked to shelter in place are be...   \n",
              "3   6     NaN      NaN   people receive wildfires evacuation orders in...   \n",
              "4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11d960ad-fc07-42c9-8beb-b030e7086b9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>our deeds are the reason of this earthquake ma...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>all residents asked to shelter in place are be...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>people receive wildfires evacuation orders in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>just got sent this photo from ruby alaska as s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11d960ad-fc07-42c9-8beb-b030e7086b9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11d960ad-fc07-42c9-8beb-b030e7086b9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11d960ad-fc07-42c9-8beb-b030e7086b9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a text cleaner to remove punctuation and random characters"
      ],
      "metadata": {
        "id": "lMLbmlXcd3Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleantext(text):\n",
        "    \n",
        "    text = str(text).lower()  \n",
        "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text) \n",
        "    text = re.sub(r\"\\s+\", \" \", text)  \n",
        "    text = re.sub(r\"\\w+…|…\", \"\", text)  \n",
        "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  \n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
        "    text = re.sub('[^a-zA-Z1-9]+', ' ', str(text))\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "vAnQY0D2Yhpd"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'] = train_df.text.apply(cleantext)\n",
        "test_df['text'] = test_df.text.apply(cleantext)"
      ],
      "metadata": {
        "id": "PQdcR2ITaV0v"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess training and test text for use in the model. \n",
        "\n",
        "### This will involve creating tokens, encoding words with numbers, creating sequences, and padding the sequences with zeros so that all sequences are the same size."
      ],
      "metadata": {
        "id": "YA-vDNvbeINo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "test_df = pd.read_csv('test.csv')\n",
        "train_labels = train_df.target.values\n",
        "\n",
        "tokenizer1 = Tokenizer(num_words=75000, oov_token = '<OOV>')\n",
        "tokenizer1.fit_on_texts(train_df.text)\n",
        "words = tokenizer1.word_index\n",
        "sequences = tokenizer1.texts_to_sequences(train_df.text)\n",
        "pad = pad_sequences(sequences, padding='post', truncating = 'post', maxlen =150)\n",
        "\n",
        "test_seq = tokenizer1.texts_to_sequences(test_df.text)\n",
        "test_pad = pad_sequences(test_seq,padding = 'post', maxlen=150)"
      ],
      "metadata": {
        "id": "dRlyBJ5S_hZn"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Frequency"
      ],
      "metadata": {
        "id": "BVoMb6EliLoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict = dict(tokenizer1.word_counts)\n",
        "\n",
        "keys = list(word_dict.keys())\n",
        "values = list(word_dict.values())\n",
        "sorted_value_index = np.argsort(values)\n",
        "sorted_dict = {keys[i]: values[i] for i in sorted_value_index[::-1]}\n",
        "\n",
        "sorted_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ftRu2W9f2F3",
        "outputId": "c0582324-aa5e-472e-8876-793d1e4d132f"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 3276,\n",
              " 'a': 2193,\n",
              " 'in': 1982,\n",
              " 'to': 1950,\n",
              " 'of': 1829,\n",
              " 'and': 1423,\n",
              " 'i': 1405,\n",
              " 'is': 944,\n",
              " 'for': 895,\n",
              " 'on': 860,\n",
              " 'you': 810,\n",
              " 'my': 678,\n",
              " 'it': 579,\n",
              " 'with': 574,\n",
              " 'that': 567,\n",
              " 'at': 543,\n",
              " 'by': 527,\n",
              " 'this': 478,\n",
              " 'from': 420,\n",
              " 'be': 408,\n",
              " 'are': 403,\n",
              " 'have': 386,\n",
              " 'was': 385,\n",
              " 'like': 347,\n",
              " 'as': 330,\n",
              " 'up': 329,\n",
              " 'me': 323,\n",
              " 'just': 322,\n",
              " 'but': 317,\n",
              " 'so': 317,\n",
              " 'amp': 300,\n",
              " 'im': 299,\n",
              " 'not': 297,\n",
              " 'your': 293,\n",
              " 'out': 271,\n",
              " 'its': 268,\n",
              " 'no': 262,\n",
              " 'all': 261,\n",
              " 'after': 260,\n",
              " 'will': 257,\n",
              " 'when': 255,\n",
              " 'fire': 252,\n",
              " 'an': 251,\n",
              " 'has': 250,\n",
              " 'we': 242,\n",
              " 'if': 242,\n",
              " 'get': 229,\n",
              " 'new': 226,\n",
              " 'now': 222,\n",
              " 'via': 220,\n",
              " 'more': 217,\n",
              " 'about': 214,\n",
              " 'or': 203,\n",
              " 'what': 199,\n",
              " 'one': 197,\n",
              " 'people': 196,\n",
              " 'news': 195,\n",
              " 'he': 194,\n",
              " 'they': 193,\n",
              " 'dont': 191,\n",
              " 'been': 191,\n",
              " 'how': 191,\n",
              " 'over': 189,\n",
              " 'who': 180,\n",
              " 'into': 173,\n",
              " 'do': 168,\n",
              " 'were': 167,\n",
              " 'us': 167,\n",
              " 'can': 165,\n",
              " 's': 163,\n",
              " 'video': 158,\n",
              " 'emergency': 157,\n",
              " 'disaster': 154,\n",
              " 'there': 151,\n",
              " 'police': 141,\n",
              " 'than': 139,\n",
              " 'her': 134,\n",
              " 'would': 131,\n",
              " 'body': 129,\n",
              " 'some': 129,\n",
              " 'still': 129,\n",
              " 'his': 128,\n",
              " 'burning': 120,\n",
              " 'back': 120,\n",
              " 'suicide': 119,\n",
              " 'crash': 119,\n",
              " 'storm': 119,\n",
              " 'day': 117,\n",
              " 'california': 117,\n",
              " 'time': 117,\n",
              " 'man': 115,\n",
              " 'off': 115,\n",
              " 'why': 114,\n",
              " 'them': 113,\n",
              " 'know': 113,\n",
              " 'got': 112,\n",
              " 'had': 111,\n",
              " 'buildings': 110,\n",
              " 'first': 109,\n",
              " 'rt': 107,\n",
              " 'bomb': 105,\n",
              " 'see': 105,\n",
              " 'going': 104,\n",
              " 'nuclear': 104,\n",
              " 'world': 103,\n",
              " 'cant': 102,\n",
              " 'two': 102,\n",
              " 'love': 101,\n",
              " 'fires': 100,\n",
              " 'our': 100,\n",
              " 'attack': 99,\n",
              " 'youtube': 98,\n",
              " 'year': 98,\n",
              " 'go': 97,\n",
              " 'dead': 96,\n",
              " 'killed': 96,\n",
              " 'their': 94,\n",
              " 'full': 94,\n",
              " 'car': 94,\n",
              " 'train': 93,\n",
              " 'life': 92,\n",
              " 'war': 92,\n",
              " 'old': 91,\n",
              " 'being': 90,\n",
              " 'good': 89,\n",
              " 'hiroshima': 89,\n",
              " 'only': 89,\n",
              " 'here': 89,\n",
              " 'down': 88,\n",
              " 'today': 88,\n",
              " 'families': 88,\n",
              " 'may': 88,\n",
              " 'accident': 87,\n",
              " 'say': 86,\n",
              " 'think': 85,\n",
              " 'watch': 85,\n",
              " 'u': 85,\n",
              " 'many': 84,\n",
              " 'last': 83,\n",
              " 'could': 81,\n",
              " 'want': 80,\n",
              " 'years': 80,\n",
              " 'pm': 80,\n",
              " 'did': 79,\n",
              " 'too': 79,\n",
              " 'way': 79,\n",
              " 'make': 78,\n",
              " 'home': 78,\n",
              " 'then': 78,\n",
              " 'httptco': 76,\n",
              " 'collapse': 75,\n",
              " 'because': 74,\n",
              " 'work': 74,\n",
              " 'best': 74,\n",
              " 'help': 73,\n",
              " 'look': 73,\n",
              " 'need': 72,\n",
              " 'am': 72,\n",
              " 'wildfire': 72,\n",
              " 'mass': 72,\n",
              " 'death': 72,\n",
              " 'even': 72,\n",
              " 'army': 71,\n",
              " 'really': 71,\n",
              " 'mh37': 71,\n",
              " 'lol': 70,\n",
              " 'please': 70,\n",
              " 'bombing': 70,\n",
              " 'take': 70,\n",
              " 'let': 69,\n",
              " 'another': 69,\n",
              " 'him': 69,\n",
              " 'should': 68,\n",
              " 'those': 68,\n",
              " 'right': 68,\n",
              " 'fatal': 67,\n",
              " 'school': 67,\n",
              " 'youre': 67,\n",
              " 'hot': 66,\n",
              " 'water': 66,\n",
              " 'forest': 65,\n",
              " 'northern': 64,\n",
              " 'read': 64,\n",
              " 'much': 64,\n",
              " 'she': 63,\n",
              " 'never': 63,\n",
              " 'live': 62,\n",
              " 'obama': 62,\n",
              " 'legionnaires': 62,\n",
              " 'great': 62,\n",
              " 'bomber': 61,\n",
              " 'city': 61,\n",
              " 'wreck': 61,\n",
              " 'homes': 61,\n",
              " 'latest': 59,\n",
              " 't': 59,\n",
              " 'any': 59,\n",
              " 'every': 59,\n",
              " 'typhoon': 58,\n",
              " 'atomic': 58,\n",
              " 'god': 58,\n",
              " 'fear': 57,\n",
              " 'flood': 57,\n",
              " 'floods': 57,\n",
              " 'w': 57,\n",
              " 'thats': 57,\n",
              " 'where': 57,\n",
              " 'content': 57,\n",
              " 'flames': 57,\n",
              " 'said': 57,\n",
              " 'under': 56,\n",
              " 'shit': 56,\n",
              " 'near': 56,\n",
              " 'come': 56,\n",
              " 'ever': 56,\n",
              " 'getting': 56,\n",
              " 'feel': 55,\n",
              " 'while': 55,\n",
              " 'japan': 54,\n",
              " 'top': 54,\n",
              " 'damage': 53,\n",
              " 'most': 53,\n",
              " 'since': 53,\n",
              " 'well': 53,\n",
              " 'everyone': 53,\n",
              " 'oil': 53,\n",
              " 'hit': 53,\n",
              " 'cross': 53,\n",
              " 'state': 52,\n",
              " 'before': 52,\n",
              " 'injured': 52,\n",
              " 'military': 52,\n",
              " 'hope': 52,\n",
              " 'found': 52,\n",
              " 'without': 51,\n",
              " 'ass': 51,\n",
              " 'coming': 51,\n",
              " 'next': 51,\n",
              " 'during': 51,\n",
              " 'weather': 51,\n",
              " 'these': 50,\n",
              " 'earthquake': 50,\n",
              " 'night': 50,\n",
              " 'malaysia': 50,\n",
              " 'little': 50,\n",
              " 'debris': 50,\n",
              " 'stop': 50,\n",
              " 'truck': 50,\n",
              " 'evacuation': 50,\n",
              " 'flooding': 50,\n",
              " 'which': 49,\n",
              " 'black': 49,\n",
              " 'plan': 49,\n",
              " 'smoke': 48,\n",
              " 'set': 48,\n",
              " 'area': 48,\n",
              " 'wounded': 48,\n",
              " 'cause': 48,\n",
              " 'times': 48,\n",
              " 'face': 47,\n",
              " 'severe': 47,\n",
              " 'wild': 47,\n",
              " 'food': 47,\n",
              " 'run': 47,\n",
              " 'movie': 47,\n",
              " 'heat': 47,\n",
              " 'through': 47,\n",
              " 'thunderstorm': 47,\n",
              " 'confirmed': 47,\n",
              " 'high': 46,\n",
              " 'reddit': 46,\n",
              " 'lightning': 46,\n",
              " 'explosion': 46,\n",
              " 'check': 46,\n",
              " 'made': 46,\n",
              " 'fucking': 46,\n",
              " 'looks': 46,\n",
              " 'sinking': 46,\n",
              " 'always': 46,\n",
              " 'natural': 45,\n",
              " 'theres': 45,\n",
              " 'thunder': 45,\n",
              " 'bloody': 45,\n",
              " 'bad': 45,\n",
              " 'weapons': 45,\n",
              " 'rain': 45,\n",
              " 'bags': 44,\n",
              " 'liked': 44,\n",
              " 'until': 44,\n",
              " 'warning': 44,\n",
              " 'house': 44,\n",
              " 'also': 44,\n",
              " 'devastated': 44,\n",
              " 'family': 44,\n",
              " 'refugees': 44,\n",
              " 'injuries': 44,\n",
              " 'says': 44,\n",
              " 'hes': 44,\n",
              " 'weapon': 44,\n",
              " 'services': 44,\n",
              " 'fall': 44,\n",
              " 'blood': 44,\n",
              " 'end': 44,\n",
              " 'loud': 43,\n",
              " 'evacuate': 43,\n",
              " 'missing': 43,\n",
              " 'spill': 43,\n",
              " 'gonna': 43,\n",
              " 'head': 43,\n",
              " 'free': 43,\n",
              " 'change': 43,\n",
              " 'ive': 43,\n",
              " 'boy': 43,\n",
              " 'screaming': 43,\n",
              " 'trapped': 42,\n",
              " 'again': 42,\n",
              " 'summer': 42,\n",
              " 'red': 42,\n",
              " 'collided': 42,\n",
              " 'failure': 42,\n",
              " 'sinkhole': 42,\n",
              " 'murder': 42,\n",
              " 'm': 42,\n",
              " 'air': 42,\n",
              " 'wreckage': 41,\n",
              " 'panic': 41,\n",
              " 're': 41,\n",
              " 'terrorism': 41,\n",
              " 'post': 41,\n",
              " 'attacked': 41,\n",
              " 'saudi': 41,\n",
              " 'released': 41,\n",
              " 'deaths': 41,\n",
              " 'derailment': 41,\n",
              " 'migrants': 41,\n",
              " 'destroy': 41,\n",
              " 'save': 41,\n",
              " 'survive': 41,\n",
              " 'photo': 41,\n",
              " 'whole': 41,\n",
              " 'outbreak': 41,\n",
              " 'bag': 41,\n",
              " 'girl': 40,\n",
              " 'rescue': 40,\n",
              " 'collision': 40,\n",
              " 'fatalities': 40,\n",
              " 'tonight': 40,\n",
              " 'does': 40,\n",
              " 'someone': 40,\n",
              " 'hurricane': 40,\n",
              " 'bridge': 40,\n",
              " 'big': 40,\n",
              " 'destroyed': 40,\n",
              " 'explode': 40,\n",
              " 'island': 39,\n",
              " 'rescued': 39,\n",
              " 'road': 39,\n",
              " 'survived': 39,\n",
              " 'ill': 39,\n",
              " 'terrorist': 39,\n",
              " 'ambulance': 39,\n",
              " 'trauma': 39,\n",
              " 'twister': 39,\n",
              " 'charged': 39,\n",
              " 'around': 39,\n",
              " 'wrecked': 39,\n",
              " 'week': 39,\n",
              " 'lives': 39,\n",
              " 'breaking': 39,\n",
              " 'burned': 39,\n",
              " 'fuck': 39,\n",
              " 'destruction': 39,\n",
              " 'keep': 39,\n",
              " 'service': 38,\n",
              " 'sunk': 38,\n",
              " 'away': 38,\n",
              " 'other': 38,\n",
              " 'county': 38,\n",
              " 'long': 38,\n",
              " 'drought': 38,\n",
              " 'game': 38,\n",
              " 'story': 38,\n",
              " 'survivors': 38,\n",
              " 'bombed': 38,\n",
              " 'rescuers': 38,\n",
              " 'self': 38,\n",
              " 'ruin': 38,\n",
              " 'hazard': 38,\n",
              " 'hostages': 38,\n",
              " 'catastrophe': 38,\n",
              " 'landslide': 38,\n",
              " 'call': 38,\n",
              " 'dust': 38,\n",
              " 'against': 38,\n",
              " 'deluge': 38,\n",
              " 'battle': 37,\n",
              " 'injury': 37,\n",
              " 'boat': 37,\n",
              " 'danger': 37,\n",
              " 'airplane': 37,\n",
              " 'structural': 37,\n",
              " 'curfew': 37,\n",
              " 'white': 37,\n",
              " 'collapsed': 37,\n",
              " 'august': 37,\n",
              " 'investigators': 37,\n",
              " 'harm': 37,\n",
              " 'put': 37,\n",
              " 'show': 37,\n",
              " 'mudslide': 37,\n",
              " 'whirlwind': 37,\n",
              " 'violent': 37,\n",
              " 'hail': 37,\n",
              " 'massacre': 37,\n",
              " 'real': 37,\n",
              " 'devastation': 37,\n",
              " 'phone': 37,\n",
              " 'wind': 37,\n",
              " 'quarantine': 37,\n",
              " 'bus': 37,\n",
              " 'armageddon': 37,\n",
              " 'half': 37,\n",
              " 'engulfed': 36,\n",
              " 'ok': 36,\n",
              " 'sandstorm': 36,\n",
              " 'rioting': 36,\n",
              " 'riot': 36,\n",
              " 'tragedy': 36,\n",
              " 'wave': 36,\n",
              " 'windstorm': 36,\n",
              " 'screamed': 36,\n",
              " 'saw': 36,\n",
              " 'woman': 36,\n",
              " 'report': 36,\n",
              " 'came': 36,\n",
              " 'quarantined': 36,\n",
              " 'famine': 36,\n",
              " 'kills': 36,\n",
              " 'better': 36,\n",
              " 'crashed': 36,\n",
              " 'wanna': 36,\n",
              " 'mosque': 36,\n",
              " 'least': 36,\n",
              " 'thing': 36,\n",
              " 'displaced': 36,\n",
              " 'drowning': 36,\n",
              " 'hazardous': 36,\n",
              " 'suspect': 36,\n",
              " 'things': 36,\n",
              " 'heard': 35,\n",
              " 'traumatised': 35,\n",
              " 'light': 35,\n",
              " 'hundreds': 35,\n",
              " 'bleeding': 35,\n",
              " 'exploded': 35,\n",
              " 'iran': 35,\n",
              " 'casualties': 35,\n",
              " 'blown': 35,\n",
              " 'women': 35,\n",
              " 'derail': 35,\n",
              " 'past': 35,\n",
              " 'horrible': 35,\n",
              " 'chemical': 35,\n",
              " 'stock': 35,\n",
              " 'cliff': 35,\n",
              " 'anniversary': 35,\n",
              " 'fedex': 35,\n",
              " 'oh': 35,\n",
              " 'apocalypse': 35,\n",
              " 'national': 35,\n",
              " 'heart': 35,\n",
              " 'update': 35,\n",
              " 'drown': 34,\n",
              " 'lava': 34,\n",
              " 'part': 34,\n",
              " 'bang': 34,\n",
              " 'd': 34,\n",
              " 'catastrophic': 34,\n",
              " 'blew': 34,\n",
              " 'derailed': 34,\n",
              " 'hijacker': 34,\n",
              " 'desolation': 34,\n",
              " 'electrocuted': 34,\n",
              " 'crush': 34,\n",
              " 'group': 34,\n",
              " 'inundated': 34,\n",
              " 'wounds': 34,\n",
              " 'detonate': 34,\n",
              " 'twitter': 34,\n",
              " 'went': 33,\n",
              " 'panicking': 33,\n",
              " 'left': 33,\n",
              " 'zone': 33,\n",
              " 'collide': 33,\n",
              " 'evacuated': 33,\n",
              " 'lot': 33,\n",
              " 'ebay': 33,\n",
              " 'land': 33,\n",
              " 'razed': 33,\n",
              " 'reunion': 33,\n",
              " 'affected': 33,\n",
              " 'hostage': 33,\n",
              " 'caused': 33,\n",
              " 'power': 33,\n",
              " 'flattened': 33,\n",
              " 'id': 33,\n",
              " 'isis': 33,\n",
              " 'bagging': 33,\n",
              " 'bioterror': 33,\n",
              " 'trouble': 33,\n",
              " 'plane': 33,\n",
              " 'demolish': 33,\n",
              " 'song': 32,\n",
              " 'market': 32,\n",
              " 'pandemonium': 32,\n",
              " 'thank': 32,\n",
              " 'meltdown': 32,\n",
              " 'th': 32,\n",
              " 'river': 32,\n",
              " 'kill': 32,\n",
              " 'doing': 32,\n",
              " 'soon': 32,\n",
              " 'baby': 32,\n",
              " 'send': 32,\n",
              " 'tomorrow': 32,\n",
              " 'hijacking': 32,\n",
              " 'calgary': 32,\n",
              " 'possible': 32,\n",
              " 'sure': 32,\n",
              " 'screams': 32,\n",
              " 'very': 32,\n",
              " 'minute': 32,\n",
              " 'must': 32,\n",
              " 'demolition': 32,\n",
              " 'tornado': 32,\n",
              " 'something': 32,\n",
              " 'volcano': 32,\n",
              " 'r': 32,\n",
              " 'lets': 31,\n",
              " 'n': 31,\n",
              " 'didnt': 31,\n",
              " 'security': 31,\n",
              " 'pkk': 31,\n",
              " 'cool': 31,\n",
              " 'arson': 31,\n",
              " 'goes': 31,\n",
              " 'india': 31,\n",
              " 'government': 31,\n",
              " 'traffic': 31,\n",
              " 'thought': 31,\n",
              " 'detonated': 31,\n",
              " 'detonation': 31,\n",
              " 'due': 31,\n",
              " 'care': 31,\n",
              " 'south': 31,\n",
              " 'officials': 31,\n",
              " 'use': 31,\n",
              " 'longer': 31,\n",
              " 'three': 31,\n",
              " 'murderer': 31,\n",
              " 'annihilated': 31,\n",
              " 'obliterated': 31,\n",
              " 'responders': 31,\n",
              " 'building': 30,\n",
              " 'sound': 30,\n",
              " 'used': 30,\n",
              " 'obliteration': 30,\n",
              " 'obliterate': 30,\n",
              " 'drowned': 30,\n",
              " 'beautiful': 30,\n",
              " 'yet': 30,\n",
              " 'airport': 30,\n",
              " 'kids': 30,\n",
              " 'officer': 30,\n",
              " 'fan': 30,\n",
              " 'crushed': 30,\n",
              " 'eyewitness': 30,\n",
              " 'blast': 30,\n",
              " 'st': 30,\n",
              " 'tsunami': 30,\n",
              " 'same': 30,\n",
              " 'cyclone': 30,\n",
              " 'fatality': 30,\n",
              " 'thanks': 30,\n",
              " 'issues': 30,\n",
              " 'shoulder': 30,\n",
              " 'demolished': 30,\n",
              " 'sirens': 30,\n",
              " 'prebreak': 30,\n",
              " 'blazing': 30,\n",
              " 'nothing': 30,\n",
              " 'media': 29,\n",
              " 'men': 29,\n",
              " 'ur': 29,\n",
              " 'casualty': 29,\n",
              " 'music': 29,\n",
              " 'few': 29,\n",
              " 'fight': 29,\n",
              " 'remember': 29,\n",
              " 'shooting': 29,\n",
              " 'stretcher': 29,\n",
              " 'fun': 29,\n",
              " 'whats': 29,\n",
              " 'ablaze': 29,\n",
              " 'stay': 29,\n",
              " 'already': 29,\n",
              " 'hellfire': 29,\n",
              " 'done': 29,\n",
              " 'offensive': 29,\n",
              " 'upheaval': 29,\n",
              " 'making': 29,\n",
              " 'hours': 29,\n",
              " 'hijack': 28,\n",
              " 'actually': 28,\n",
              " 'far': 28,\n",
              " 'believe': 28,\n",
              " 'died': 28,\n",
              " 'start': 28,\n",
              " 'second': 28,\n",
              " 'hell': 28,\n",
              " 'seismic': 28,\n",
              " 'bush': 28,\n",
              " 'words': 28,\n",
              " 'leave': 28,\n",
              " 'policy': 28,\n",
              " 'sue': 28,\n",
              " 'rainstorm': 28,\n",
              " 'electrocute': 28,\n",
              " 'having': 28,\n",
              " 'turkey': 28,\n",
              " 'israeli': 28,\n",
              " '16yr': 28,\n",
              " 'person': 28,\n",
              " 'days': 28,\n",
              " 'wake': 28,\n",
              " 'inside': 28,\n",
              " 'guys': 28,\n",
              " 'both': 27,\n",
              " 'yes': 27,\n",
              " 'declares': 27,\n",
              " 'lab': 27,\n",
              " 'site': 27,\n",
              " 'china': 27,\n",
              " 'shot': 27,\n",
              " 'avalanche': 27,\n",
              " 'trying': 27,\n",
              " 'plans': 27,\n",
              " 'doesnt': 27,\n",
              " 'nearby': 27,\n",
              " 'play': 27,\n",
              " 'such': 27,\n",
              " 'reactor': 27,\n",
              " 'stand': 27,\n",
              " 'bc': 27,\n",
              " 'north': 27,\n",
              " 'line': 27,\n",
              " 'peace': 26,\n",
              " 'rubble': 26,\n",
              " 'deluged': 26,\n",
              " 'wait': 26,\n",
              " 'snowstorm': 26,\n",
              " 'low': 26,\n",
              " 'wont': 26,\n",
              " 'deal': 26,\n",
              " 'place': 26,\n",
              " 'swallowed': 26,\n",
              " 'islam': 26,\n",
              " 'nowplaying': 26,\n",
              " 'blight': 26,\n",
              " 'children': 26,\n",
              " 'find': 26,\n",
              " 'mp': 26,\n",
              " 'ago': 26,\n",
              " 'horror': 26,\n",
              " 'brown': 26,\n",
              " 'health': 26,\n",
              " 'yourself': 26,\n",
              " 'gets': 26,\n",
              " 'anything': 26,\n",
              " 'yeah': 26,\n",
              " 'memories': 26,\n",
              " 'conclusively': 25,\n",
              " 'abc': 25,\n",
              " 'job': 25,\n",
              " 'rise': 25,\n",
              " 'helicopter': 25,\n",
              " 'watching': 25,\n",
              " 'tell': 25,\n",
              " 'maybe': 25,\n",
              " 'american': 25,\n",
              " 'west': 25,\n",
              " 'own': 25,\n",
              " 'saipan': 25,\n",
              " 'bigger': 25,\n",
              " 'data': 25,\n",
              " 'history': 25,\n",
              " 'almost': 25,\n",
              " 'order': 25,\n",
              " 'business': 25,\n",
              " 'la': 25,\n",
              " 'outside': 25,\n",
              " 'pic': 25,\n",
              " 'aircraft': 25,\n",
              " 'photos': 25,\n",
              " 'anyone': 24,\n",
              " 'c': 24,\n",
              " 'crews': 24,\n",
              " 'america': 24,\n",
              " 'team': 24,\n",
              " 'street': 24,\n",
              " 'e': 24,\n",
              " 'literally': 24,\n",
              " 'b': 24,\n",
              " 'siren': 24,\n",
              " 'pick': 24,\n",
              " 'tv': 24,\n",
              " 'searching': 24,\n",
              " 'bestnaijamade': 24,\n",
              " 'transport': 24,\n",
              " 'money': 24,\n",
              " 'makes': 24,\n",
              " 'bodies': 24,\n",
              " 'bar': 24,\n",
              " 'happy': 24,\n",
              " 'hey': 24,\n",
              " 'projected': 24,\n",
              " 'hear': 24,\n",
              " 'waves': 24,\n",
              " 'control': 24,\n",
              " 'support': 24,\n",
              " 'bioterrorism': 24,\n",
              " 'die': 24,\n",
              " 'side': 24,\n",
              " 'desolate': 23,\n",
              " 'myself': 23,\n",
              " 'manslaughter': 23,\n",
              " 'book': 23,\n",
              " 'soudelor': 23,\n",
              " 'amid': 23,\n",
              " 'co': 23,\n",
              " 'feeling': 23,\n",
              " 'trench': 23,\n",
              " 'might': 23,\n",
              " 'saved': 23,\n",
              " 'move': 23,\n",
              " 'center': 23,\n",
              " 'eyes': 23,\n",
              " 'gt': 23,\n",
              " 'pretty': 23,\n",
              " 'signs': 23,\n",
              " 'lost': 23,\n",
              " 'reuters': 23,\n",
              " 'everything': 23,\n",
              " 'finally': 23,\n",
              " 'effect': 23,\n",
              " 'annihilation': 23,\n",
              " 'probably': 23,\n",
              " 'theyre': 23,\n",
              " 'online': 23,\n",
              " 'hollywood': 23,\n",
              " 'damn': 22,\n",
              " 'caught': 22,\n",
              " 'pakistan': 22,\n",
              " 'miners': 22,\n",
              " 'wrong': 22,\n",
              " 'shes': 22,\n",
              " 'blaze': 22,\n",
              " 'country': 22,\n",
              " 'town': 22,\n",
              " 'trains': 22,\n",
              " 'image': 22,\n",
              " 'sensor': 22,\n",
              " 'child': 22,\n",
              " 'feared': 22,\n",
              " 'p': 22,\n",
              " 'spot': 22,\n",
              " 'major': 22,\n",
              " 'space': 22,\n",
              " 'costlier': 22,\n",
              " 'though': 22,\n",
              " 'crisis': 22,\n",
              " 'okay': 22,\n",
              " 'fast': 22,\n",
              " 'name': 22,\n",
              " 'leather': 22,\n",
              " 'refugio': 22,\n",
              " 'x': 22,\n",
              " 'case': 22,\n",
              " 'youth': 22,\n",
              " 'seen': 22,\n",
              " 'nearly': 22,\n",
              " 'hate': 22,\n",
              " 'once': 22,\n",
              " 'hailstorm': 21,\n",
              " 'ball': 21,\n",
              " 'texas': 21,\n",
              " 'course': 21,\n",
              " 'crazy': 21,\n",
              " 'picking': 21,\n",
              " 'heavy': 21,\n",
              " 'jobs': 21,\n",
              " 'knock': 21,\n",
              " 'sorry': 21,\n",
              " 'morning': 21,\n",
              " 'omg': 21,\n",
              " 'others': 21,\n",
              " 'ship': 21,\n",
              " 'womens': 21,\n",
              " 'flash': 21,\n",
              " 'east': 21,\n",
              " 'giant': 21,\n",
              " 'russian': 21,\n",
              " 'class': 21,\n",
              " 'banned': 21,\n",
              " 'isnt': 21,\n",
              " 'mom': 21,\n",
              " 'cars': 21,\n",
              " 'vehicle': 21,\n",
              " 'worst': 21,\n",
              " 'called': 21,\n",
              " 'needs': 21,\n",
              " 'huge': 21,\n",
              " 'level': 20,\n",
              " 'property': 20,\n",
              " 'become': 20,\n",
              " 'across': 20,\n",
              " 'lord': 20,\n",
              " 'united': 20,\n",
              " 'wow': 20,\n",
              " 'looking': 20,\n",
              " 'computers': 20,\n",
              " 'action': 20,\n",
              " 'gbbo': 20,\n",
              " 'usa': 20,\n",
              " 'uk': 20,\n",
              " 'closed': 20,\n",
              " 'mayhem': 20,\n",
              " 'listen': 20,\n",
              " 'star': 20,\n",
              " 'win': 20,\n",
              " 'gun': 20,\n",
              " 'sign': 20,\n",
              " 'chance': 20,\n",
              " 'houses': 20,\n",
              " 'poor': 20,\n",
              " 'rd': 20,\n",
              " 'meek': 20,\n",
              " 'bbc': 20,\n",
              " 'friends': 20,\n",
              " 'toddler': 20,\n",
              " 'angry': 20,\n",
              " 'ignition': 20,\n",
              " 'reason': 20,\n",
              " 'entire': 20,\n",
              " 'flag': 20,\n",
              " 'anthrax': 20,\n",
              " 'hard': 20,\n",
              " 'haha': 20,\n",
              " 'totally': 19,\n",
              " 'alone': 19,\n",
              " 'thousands': 19,\n",
              " 'pay': 19,\n",
              " 'follow': 19,\n",
              " 'appears': 19,\n",
              " 'cake': 19,\n",
              " 'temple': 19,\n",
              " 'talk': 19,\n",
              " 'blizzard': 19,\n",
              " 'emmerdale': 19,\n",
              " 'christian': 19,\n",
              " 'yours': 19,\n",
              " 'public': 19,\n",
              " 'climate': 19,\n",
              " 'drive': 19,\n",
              " 'running': 19,\n",
              " 'drake': 19,\n",
              " 'ladies': 19,\n",
              " 'view': 19,\n",
              " 'fukushima': 19,\n",
              " 'beach': 19,\n",
              " 'reports': 19,\n",
              " 'truth': 19,\n",
              " 'try': 19,\n",
              " 'pain': 19,\n",
              " 'takes': 19,\n",
              " 'learn': 19,\n",
              " 'happened': 19,\n",
              " 'centre': 19,\n",
              " 'couple': 19,\n",
              " 'russia': 19,\n",
              " 'declaration': 19,\n",
              " 'dog': 19,\n",
              " 'village': 19,\n",
              " 'large': 19,\n",
              " 'disea': 19,\n",
              " 'taken': 19,\n",
              " 'guy': 19,\n",
              " 'playing': 19,\n",
              " 'mishaps': 19,\n",
              " 'blue': 19,\n",
              " 'aug': 19,\n",
              " 'issued': 19,\n",
              " 'radio': 19,\n",
              " 'daily': 19,\n",
              " 'else': 18,\n",
              " 'mad': 18,\n",
              " 'chicago': 18,\n",
              " 'coaches': 18,\n",
              " 'flight': 18,\n",
              " 'vs': 18,\n",
              " 'reddits': 18,\n",
              " 'arsonist': 18,\n",
              " 'till': 18,\n",
              " 'israel': 18,\n",
              " 'friend': 18,\n",
              " 'wanted': 18,\n",
              " 'quiz': 18,\n",
              " 'aint': 18,\n",
              " 'ready': 18,\n",
              " 'marks': 18,\n",
              " 'cnn': 18,\n",
              " 'myanmar': 18,\n",
              " 'thursday': 18,\n",
              " 'favorite': 18,\n",
              " 'driving': 18,\n",
              " 'official': 18,\n",
              " 'mop': 18,\n",
              " 'green': 18,\n",
              " 'link': 18,\n",
              " 'muslims': 18,\n",
              " 'british': 18,\n",
              " 'front': 18,\n",
              " 'chile': 18,\n",
              " 'eye': 18,\n",
              " 'experts': 18,\n",
              " 'don': 18,\n",
              " 'virgin': 18,\n",
              " 'mount': 18,\n",
              " 'germs': 18,\n",
              " 'downtown': 18,\n",
              " 'global': 18,\n",
              " 'film': 18,\n",
              " 'insurance': 18,\n",
              " 'instead': 18,\n",
              " 'alarm': 18,\n",
              " 'bayelsa': 17,\n",
              " 'australia': 17,\n",
              " 'outrage': 17,\n",
              " 'taking': 17,\n",
              " 'patience': 17,\n",
              " 'sex': 17,\n",
              " 'wonder': 17,\n",
              " 'ppl': 17,\n",
              " 'gave': 17,\n",
              " 'sounds': 17,\n",
              " 'libya': 17,\n",
              " 'hiring': 17,\n",
              " 'following': 17,\n",
              " 'reported': 17,\n",
              " 'former': 17,\n",
              " 'turn': 17,\n",
              " 'funtenna': 17,\n",
              " 'escape': 17,\n",
              " 'ca': 17,\n",
              " 'bring': 17,\n",
              " 'gems': 17,\n",
              " 'behind': 17,\n",
              " 'four': 17,\n",
              " 'york': 17,\n",
              " 'point': 17,\n",
              " 'pradesh': 17,\n",
              " 'govt': 17,\n",
              " 'early': 17,\n",
              " 'madhya': 17,\n",
              " 'driver': 17,\n",
              " 'middle': 17,\n",
              " 'gop': 17,\n",
              " 'v': 17,\n",
              " 'moment': 17,\n",
              " 'turned': 17,\n",
              " 'ancient': 17,\n",
              " 'download': 17,\n",
              " 'true': 17,\n",
              " 'added': 17,\n",
              " 'subreddits': 17,\n",
              " 'led': 17,\n",
              " 'camp': 17,\n",
              " 'theater': 17,\n",
              " 'france': 17,\n",
              " 'comes': 17,\n",
              " 'scared': 17,\n",
              " 'landing': 17,\n",
              " 'park': 17,\n",
              " 'nagasaki': 17,\n",
              " 'party': 16,\n",
              " 'room': 16,\n",
              " 'give': 16,\n",
              " 'sea': 16,\n",
              " 'bed': 16,\n",
              " 'shows': 16,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the average length of a tweet to help with maxlen."
      ],
      "metadata": {
        "id": "fAKVAzO2eqSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "for i in range(len(sequences)):\n",
        "  total += len(sequences[i])\n",
        "print('Average number of characters: '+ str(total/len(sequences)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5kdmwVtbeD6",
        "outputId": "06c51835-5111-41f7-e158-6bab61f09a29"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of characters: 14.53684487061605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build stacked GRU RNN with a single dense layer to produce an output."
      ],
      "metadata": {
        "id": "bH_bKskae0ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api._v2.keras import activations\n",
        "from keras.layers import Flatten\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(input_dim = 75000, output_dim = 64, input_length=150))\n",
        "\n",
        "model.add(layers.Bidirectional(keras.layers.GRU(64,activation= 'tanh', dropout=.35, return_sequences=True)))\n",
        "model.add(layers.Bidirectional(keras.layers.GRU(32, activation= 'tanh', dropout=.4)))\n",
        "\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezO6RpP4fONP",
        "outputId": "3dc8f69d-a9b9-4fe3-a5d3-55a574017ab1"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_39 (Embedding)    (None, 150, 64)           4800000   \n",
            "                                                                 \n",
            " bidirectional_65 (Bidirecti  (None, 150, 128)         49920     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_66 (Bidirecti  (None, 64)               31104     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,881,089\n",
            "Trainable params: 4,881,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'binary_crossentropy', optimizer= keras.optimizers.Adam(learning_rate=.0001),\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "RE5oZ3Xv1uf9"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add callbacks and fit the model. Creating a 20% validation portion to test predictions"
      ],
      "metadata": {
        "id": "dYu0eoZXfCDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0.005, patience = 5, verbose = 1, mode = 'auto')\n",
        "mod_checkpt = ModelCheckpoint(monitor = 'val_accuracy', filepath='./best_mod.b1', verbose = 1, save_best_only= True)\n",
        "y_train = train_df.target\n",
        "cb = [early_stop, mod_checkpt]\n",
        "\n",
        "results = model.fit(pad,y_train, epochs = 20, verbose = 1, callbacks=cb, validation_split=.15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO81IrRD28F7",
        "outputId": "8ec13d7b-61d3-4c24-dc4f-c2c5896f0279"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.6790 - accuracy: 0.5716\n",
            "Epoch 1: val_accuracy improved from -inf to 0.53415, saving model to ./best_mod.b1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_98_layer_call_fn, gru_cell_98_layer_call_and_return_conditional_losses, gru_cell_99_layer_call_fn, gru_cell_99_layer_call_and_return_conditional_losses, gru_cell_101_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r203/203 [==============================] - 92s 194ms/step - loss: 0.6790 - accuracy: 0.5716 - val_loss: 0.6832 - val_accuracy: 0.5342\n",
            "Epoch 2/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.6838\n",
            "Epoch 2: val_accuracy improved from 0.53415 to 0.73730, saving model to ./best_mod.b1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_98_layer_call_fn, gru_cell_98_layer_call_and_return_conditional_losses, gru_cell_99_layer_call_fn, gru_cell_99_layer_call_and_return_conditional_losses, gru_cell_101_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r203/203 [==============================] - 32s 157ms/step - loss: 0.5844 - accuracy: 0.6838 - val_loss: 0.5405 - val_accuracy: 0.7373\n",
            "Epoch 3/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.3957 - accuracy: 0.8323\n",
            "Epoch 3: val_accuracy improved from 0.73730 to 0.74431, saving model to ./best_mod.b1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_98_layer_call_fn, gru_cell_98_layer_call_and_return_conditional_losses, gru_cell_99_layer_call_fn, gru_cell_99_layer_call_and_return_conditional_losses, gru_cell_101_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r203/203 [==============================] - 29s 144ms/step - loss: 0.3957 - accuracy: 0.8323 - val_loss: 0.5222 - val_accuracy: 0.7443\n",
            "Epoch 4/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.8832\n",
            "Epoch 4: val_accuracy did not improve from 0.74431\n",
            "203/203 [==============================] - 5s 25ms/step - loss: 0.2897 - accuracy: 0.8832 - val_loss: 0.5225 - val_accuracy: 0.7443\n",
            "Epoch 5/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9213\n",
            "Epoch 5: val_accuracy improved from 0.74431 to 0.75219, saving model to ./best_mod.b1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_98_layer_call_fn, gru_cell_98_layer_call_and_return_conditional_losses, gru_cell_99_layer_call_fn, gru_cell_99_layer_call_and_return_conditional_losses, gru_cell_101_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r203/203 [==============================] - 31s 155ms/step - loss: 0.2147 - accuracy: 0.9213 - val_loss: 0.5565 - val_accuracy: 0.7522\n",
            "Epoch 6/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9413\n",
            "Epoch 6: val_accuracy improved from 0.75219 to 0.75569, saving model to ./best_mod.b1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_98_layer_call_fn, gru_cell_98_layer_call_and_return_conditional_losses, gru_cell_99_layer_call_fn, gru_cell_99_layer_call_and_return_conditional_losses, gru_cell_101_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r203/203 [==============================] - 29s 142ms/step - loss: 0.1649 - accuracy: 0.9413 - val_loss: 0.5923 - val_accuracy: 0.7557\n",
            "Epoch 7/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9573\n",
            "Epoch 7: val_accuracy did not improve from 0.75569\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.1272 - accuracy: 0.9573 - val_loss: 0.6319 - val_accuracy: 0.7504\n",
            "Epoch 8/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9657\n",
            "Epoch 8: val_accuracy did not improve from 0.75569\n",
            "203/203 [==============================] - 4s 22ms/step - loss: 0.1024 - accuracy: 0.9657 - val_loss: 0.6878 - val_accuracy: 0.7417\n",
            "Epoch 9/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9713\n",
            "Epoch 9: val_accuracy did not improve from 0.75569\n",
            "203/203 [==============================] - 5s 23ms/step - loss: 0.0858 - accuracy: 0.9713 - val_loss: 0.7212 - val_accuracy: 0.7461\n",
            "Epoch 10/20\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9731\n",
            "Epoch 10: val_accuracy did not improve from 0.75569\n",
            "203/203 [==============================] - 4s 21ms/step - loss: 0.0747 - accuracy: 0.9731 - val_loss: 0.8305 - val_accuracy: 0.7093\n",
            "Epoch 10: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model('./best_mod.b1')\n",
        "\n",
        "pred = model.predict(test_pad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8oY85jNOpb2",
        "outputId": "0dfe7ce4-8eaa-436f-c1e5-848ce999fce2"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 1s 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame(pred.round(), test_df.id, columns = ['Prediction']).reset_index()"
      ],
      "metadata": {
        "id": "ex8M13oGO8sa"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.asarray(test_df.id)\n",
        "p_lst = []\n",
        "for p in pred:\n",
        "  p_lst.append(p[0])"
      ],
      "metadata": {
        "id": "sJfY6e-OWYzv"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.DataFrame( p_lst,test_df.id.values).reset_index()\n",
        "final_df.to_csv('./pred1')"
      ],
      "metadata": {
        "id": "uHUnaZNnXib8"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My submission scored 74% accuracy on the test data. I kept my model as simple as I could with a conservative learning rate. The GRU model performed better than LSTM model.\n",
        "\n",
        "### I thought about removing stopwords from the text which may have given me a little better result. I will likely try this at a later time. The keras vectorizer and tokenizer were easiest to use with a keras model.\n",
        "\n",
        "### The end result without removing stopwords was better than I expected."
      ],
      "metadata": {
        "id": "t_VBq5l_jFlf"
      }
    }
  ]
}